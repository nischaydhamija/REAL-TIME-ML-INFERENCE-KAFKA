databases:
  - name: database
    databaseName: ml_inference_db
    user: ml_user
    plan: free

services:
  - type: web
    name: ml-api
    env: docker
    dockerfilePath: ./docker/Dockerfile.api
    healthCheckPath: /health
    plan: free
    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: database
          property: connectionString
      - key: KAFKA_BOOTSTRAP_SERVERS
        value: localhost:9092
      - key: KAFKA_TOPIC
        value: ml_predictions
      - key: MODEL_PATH
        value: models/trained_model.joblib
      - key: API_TITLE
        value: Real-Time ML Inference API
      - key: API_VERSION
        value: 1.0.0
      - key: LOG_LEVEL
        value: INFO
      - key: ENVIRONMENT
        value: production
      - key: PORT
        value: 8000
    domains:
      - ml-inference-api.onrender.com

  - type: worker
    name: ml-consumer
    env: docker
    dockerfilePath: ./docker/Dockerfile.consumer
    plan: free
    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: database
          property: connectionString
      - key: KAFKA_BOOTSTRAP_SERVERS
        value: localhost:9092
      - key: KAFKA_TOPIC
        value: ml_predictions
      - key: KAFKA_GROUP_ID
        value: ml_consumer_group
      - key: MODEL_PATH
        value: models/trained_model.joblib
      - key: LOG_LEVEL
        value: INFO
      - key: ENVIRONMENT
        value: production

  - type: worker
    name: data-producer
    env: docker
    dockerfilePath: ./docker/Dockerfile.producer
    plan: free
    envVars:
      - key: KAFKA_BOOTSTRAP_SERVERS
        value: localhost:9092
      - key: KAFKA_TOPIC
        value: ml_predictions
      - key: PRODUCER_INTERVAL
        value: 10
      - key: LOG_LEVEL
        value: INFO
      - key: ENVIRONMENT
        value: production